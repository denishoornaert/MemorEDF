\section{Design Goals and Overview}

%% General introduction about the SchIM
In this section, we introduce the proposed \schim design and describe
the overarching goals of this work. We then provide a bird's-eye view
of the \schim organization and principles of operation.

\subsection{Design Goals}
As briefly surveyed in Section~\ref{sec:relwork}, there have been
numerous proposals for better memory controllers and approaches to
manage memory traffic in modern multi-core embedded platforms. With
respect to the existing literature, the purpose of this work is
twofold. First, we want to demonstrate that scheduling CPU-originated
memory traffic at the granularity of individual transactions is
possible in PS-PL platforms. Second, and more importantly, we want to
provide an infrastructure that is generic and extensible enough for
the broader research community to adopt and foster a new chapter on
PL-assisted memory scheduling. With this in mind, we establish the
following goals.

\par{\bf Extensible memory scheduling infrastructure.} First and
foremost, the \schim has been designed with modularity and
extensibility in mind. We separate the functionalities that concern
handling, queuing, selection, and forwarding of memory requests inside
our infrastructure. Moreover, we design our \schim to be able to
simultaneously support multiple memory scheduling policies. A simple,
standardized interface is provided to define new memory scheduling
policies without impacting the design of the rest of the \schim. We
discuss in Section~\ref{sec:newpol} the main steps required to
implement a new scheduling policy in the \schim.

\par{\bf Runtime configuration and transparency.} We want the \schim
to be a powerful supporting infrastructure to evaluate, compare, and
contrast memory scheduling policies. As such, we strive to provide (1)
runtime reconfigurability and (2) operational transparency. By
allowing memory scheduling policies to be switched at runtime, it is
possible to rapidly identify desirable configuration parameters or to
tune the adopted scheduling policy to workload criticality and memory
intensiveness. For this purpose, the \schim exposes a memory-mapped
configuration interface where all the operational parameters can be
changed at runtime. At the same time, we want to ensure that the
applications and the (real-time) operating system under analysis need
not to be modified to use the \schim. Hence, we propose the use of a
thin virtualization layer to selectively route memory traffic through
the \schim without changes to the binary of OS kernel and
applications.

\par{\bf Realistic performance with experimental policies.} One of the
limiting factors of research on memory scheduling policies is the
ability to construct evidence of performance improvements with
realistic workload. Proposing a new memory scheduling policy is
traditionally done with either a simulated setup, or with a
full-system soft-core implementation. In both cases, the timescale of
experiment collection is orders of magnitude slower than what it would
be in a real systems. This forces the use of unrealistically
lightweight workload and limits the repeatability of experiments of
scale. Furthermore, the multi-fold drop in performance excludes the
possibility of adopting experimental memory scheduling policies for a
production-ready system. Conversely, we envision that our \schim will
be an intermediate step in the transition path to production of
research-seeded ideas. Additionally, as PS-PL platforms mature and the
interplay of PL and memory resources improves, a \schim-like design
could be the way to go for mission-reconfigurable, upgradable embedded
systems.

\subsection{Design Overview}
As previously mentioned, the \schim leverages the PLIM
approach. CPU-originated main memory transactions are re-routed
through the programmable logic and scheduled by the \schim according
to a flexible and configurable policy. The result is that the timing
of memory transactions generated by real-time applications can be
carefully determined and reasoned upon. Because the \schim follows a
PLIM approach, transactions can be selectively sent to the \schim for
scheduling. It is always possible, however, to dynamically exclude the
\schim and instead route transactions directly to main memory. For the
purpose of this paper, however, we mainly consider a setup in which
all the CPU-generated memory transactions are handled by the \schim.

%% First and foremost, SchIM follows a selective and dynamic re-routing
%% policy settled at the run-time, meaning that provided a proper routing
%% configuration interface, SchIM can be bypassed if unacceptable
%% overhead for some applications is detected by going through the PL
%% following the Loop-Back routing strategy. Additionally, SchIM,
%% provides a safe production-ready environment on COTS platforms where
%% one can enfroce and test any proposed scheduling policy at the level
%% of the transaction altogether by the real hardware. Memory scheduling
%% traditionally done via hardware modifications at the controller level,
%% or solely via software. Providing SchIM, we are now able to transcend
%% this on intact COTS to conduct an analysis of feasibility, challenges
%% and performance.

Figure~\ref{fig:block_diagram} provides an overview of the location of
the \schim within the main components of the target platform. The
highlighted {\bf COLOR} path depicts the route that memory requests
originated by the CPUs follow when the SchIM is used. Application
memory requests can reach the \schim through multiple
interfaces. Without loss of generality, we consider a \schim instance
with two arrival lanes, which are labeled as \axiin{1} and \axiin{2}
in Figure~\ref{fig:block_diagram}. The \schim then forwards the
received transactions towards main memory through the \axiout{3}
interface. A more detailed view of the \schim module is provided in
Figure~\ref{fig:MemorEDF_module_schema} where the same convention is
used to identify input and output ports. In addition, as shown in
Figure~\ref{fig:MemorEDF_module_schema}, a fourth \axiconf{4} port is
used to configure the \schim module from the PS.

%% The objective of the module is to arbitrate the access of the bus to
%% the main memory between the different cores of the PS side at the
%% transaction level by enforcing a given policy.  Roughly, the module
%% receives transactions from the PS side, acknowledges them and finally
%% repeat them with the main memory as destination.  The exact order in
%% which inter-core transactions are being repeated is decided by an
%% embedded on-chip hardware scheduler.

\begin{figure}
  \centering
  \input{tikz/SchIM_overview.tex}
  \caption{High-level block design of the proposed \schim
    module. \emph{RM: we need to replace this picture with one that
      provides more details about the main blocks of the platforms. We
      also do not need to include the bleacher as it is not
      fundamental for the operation of the \schim.}}
  \label{fig:SchIM_overview_schema}
\end{figure}


%% Micro-architecture and first list of modules

\subsection{Micro-architecture}
\label{subsec:micro-arch}

The \schim module is composed of a number of sub-modules grouped into
three different domains, namely (i) the \emph{interfacing domain},
(ii) the \emph{queuing domain}, and (iii) the \emph{scheduling
  domain}.

\par{\bf The interfacing domain} encompasses the sub-modules in charge
of interfacing the core logic of the \schim with the rest of the
system using the AXI protocol.  This domain is comprised of three
sub-modules. These are (i) the \emph{packetizer}, (ii) the
\emph{serializer}, and (iii) the previously mentioned configuration
interface.

The PS-facing end of the {\bf packetizer} offers an AXI slave port to
accept new incoming transactions. Upon receipt, this module transforms
each transaction into an equivalent \emph{packet} that can be queued
and scheduled by the rest of the \schim. Packetization of AXI
transactions is necessary to be able to store transactions that are
serial by nature.  Indeed, a standard AXI transaction is composed of
one address phase (AR or AW channel) followed by a data phase (R or W
channel) which can be itself composed of multiple successive bursts.

In many ways, the {\bf serializer} is the dual module of the
packetizer. Its purpose is to transform the packets that encode
CPU-generated memory requests back into AXI-compliant transactions. As
such, the serializer offers a master port to the rest of the system to
be routed to the main memory controller.

%% purpose of slave and master ports, they are also in charge of
%% respectively transforming the AXI transactions into an equivalent
%% packet and to transform these packets 

\par{\bf The queuing domain} handles how packets are stored between
receipt and re-trasnmission. This domain is comprised of (i) the
dispatcher module, (ii) the transaction queues, and (iii) the selector
module.

The use of {\bf multiple transaction queues} is necessary to
differentiate the traffic of the CPU cores and perform scheduling. As
such, the \schim associates a queue to each of the active cores ---
four in the platform of reference.
%% Therefore, in order to cancel the Round Robin arbitration policy
%% applied in the PS side and in order to avoid that one high priority
%% core is stalled by a lower priority one, each core is granted a queue
%% within the \schim module. <-- This should be obvious (RM)
The queues implemented in the \schim not only act as a holding space
for in-flight memory transactions.  They also (a) provide information
to the scheduling domain regarding their current state, and (b) they
can generate a congestion control signal to the associated CPU core.

As suggested by figure \ref{fig:MemorEDF_module_schema}, transactions
are categorized and enqueued based on the source of traffic. The {\bf
  dispatcher} module performs the matching between an incoming
transaction and the destination queue. Similarly, transactions are
dequeued by the {\bf selector} module and sent directly to the output
of the \schim following the decisions of the scheduling
domain.

Additional important details about the categorization and congestion
control mechanisms enacted in the queueing domain are provided in
Section~\ref{sec:schim_implementation}.

\begin{figure}
  \centering
  \includegraphics[scale=0.08]{images/MemorEDF_module_schema.png}
  \caption{Caption}
  \label{fig:MemorEDF_module_schema}
\end{figure}

\par{\bf The scheduling domain} encompasses all the sub-modules that
enable arbitration of transactions issued by the different cores of
the PS. The modules in this domain are intended to be generic for
extensibility, albeit a first set of three template schedulers is
provided as a proof of concept.  The scheduling policies currently
implemented in the \schim are Fixed Priority (FP), Time Division
Multiple Access (TDMA), and Budget-based Traffic Shaping (TS).  Each
of the parameters required by the implemented policies --- such as the
priorities, the periods, and the budgets --- can be adjusted at
run-time via the configuration interface.

The FP scheduler allows associating a priority value to each of the
transaction queues. Pending transactions at the queues are then
forwarded out of the \schim following the user-defined priority
order. The TDMA scheduler allows associating a transmission time slot
to each of the queues expressed in PL clock cycles. The module then
builds a schedule by concatenating the per-core slots so that only
pending transactions from one queue at a time are forwarded by the
\schim. Finally, with the TS scheduler, it is possible to associate a
maximum rate at which transactions from each queue are forwarded by
the \schim. 

%% Finally, the scheduling domain is also the one in charge of the
%% control of the remaining \schim module, driving and selecting the
%% adequate signals and ensuring the coherence and integrity of the data.

%% \subsection{Modules Overview}

%% \par{\bf Packetizer:} The packetizer module transforms transactions on
%% the asynchronous AXI bus into schedulable entities to be queued at any
%% of the \schim queues.

