\section{Discussion \new{and Limitations}}
\label{sec:discussion}
%When comparing the throughput that is experienced by the cores, the normal route always provides a higher throughput in comparison to the the loop-back-based path.
%However, when only comparing those loop-back-based paths, we can see that the price to pay for having \schim is largely compensated by the memory isolation the latter provides.
%In general, redirecting the cores traffic toward the PL side with \schim is interesting in combination with techniques such as  \textit{address bleaching} and \textit{zero-copy recoloring}.
\removed{
Even though the throughput offered by the \emph{normal route} is higher, the
authors argue that comparing the latter's raw performance against \schim is unfair.
Redirecting the CPU-originated memory traffic through the PL side has a cost.
However, this cost is mainly linked to the implementation and the platform capabilities, elements that can be improved by optimization as well as a selection of more aggressive platforms.
The important aspect brought by the proposed framework, \schim, is its capability to individually manipulate memory transactions, opening the door to the study of novel memory scheduling policies.
The PL-to-PS feedback is an interesting regulating mechanism. Moreover, it is entirely transparent to the hypervisor's inmates and uses the fastest route possible to communicate with the PS side. Nonetheless, this feedback mechanism is coarse. Inset 1 of Figure \ref{fig:schim_behaviour_tdma} highlights perfectly this problem. Even though all the queues have been assigned a threshold of 4, the latter is often exceeded. The worst-case being queue 3 exceeding the threshold by 2 on the right side of the plot.
The thresholds used for the FIQ regulation requires to be fine-tuned manually by the user. Future extensions of the \schim will explore the implementation of schedulers capable of dynamically adapting the threshold to maximize the performance and improve the core isolation.
}

\new{ By design, the PLiM module proposed in this paper, the \schim,
  centralizes the memory traffic and its scheduling. A centralized
  design makes sense on the specific target platform because there
  exist only one memory controller and thus a single path between the
  LLC and the DRAM controller. In systems where multiple paths between
  the processing units and the memory controllers exist, for instance
  when multiple controllers and channels are present, a decentralized
  design is to be preferable to better exploit the available memory
  parallelism. In such platforms, a possible avenue could be
  instantiating multiple \schim modules, roughly one per channel, and
  introducing appropriate out-of-band signaling between the modules
  for coordination off the critical path.

  As we mentioned in Section~\ref{subsection:considered-architecture},
  our setup includes the Jailhouse partitioning hypervisor.  While the
  \schim module does not strictly require the PS side to use a
  hypervisor, Jailhouse has been extensively used for the evalution as
  it provides convenient features to control physical memory
  allocation.  For instance, the support for page coloring has been
  used to both partition the LLC space and to easily identify the
  owner of each memory transactions in the \schim (as presented in
  Section~\ref{sec:traffic-accounting}).  However, instead of
  enforcing cache partitioning, one could instead identify the
  ownership of memory transactions by extracting a different subset of
  address bits. For instance, if the physical memory allocated to
  different partitions is not interleaved, then the most significant
  bits of the address can be used to perform traffic accounting. In
  addition, the IPA address virtualization is convenient to
  transparently redirect the memory traffic of the application
  partitions through the PL side, even if they are initially booted
  through the normal route.  Finally, the cores throttling mechanism
  (see Section~\ref{sec:pl-to-ps-feedback}) via the FIQs can be
  implemented at EL3 (Secure Monitor) or in the individual guest OS's
  instead (EL1). Implementing FIQ handling in the hypervisor (EL2),
  however, has the advantage of not requiring any change in the guest
  OS's, as well as not requiring a full switch into secure mode
  compared to an implementation at EL3.

  On the same note, provided that the FIQ lines are not used by the
  inmates, the feedback regulation mechanism is entirely transparent
  to the guest OS's (or even for bare-metal applications) and
  introduces minimum overhead. The Linux kernel do not use FIQs, and
  the same goes for typical RTOS's. Nonetheless, it must be
  acknowledged that defining a FIQ handler to be used for CPU
  throttling might interfere with (and be interfered by) the latency
  of FIQ handling in guest OS's that rely on the same
  functionality. This is mainly because FIQ handling is
  non-preemptive. We also recognize that the PL-to-PS feedback
  mechanism is relatively coarse. Inset 1 of
  Figure~\ref{fig:schim_behaviour_tdma} highlights this problem. Even
  though all the queues have been assigned a threshold of 4, the
  threshold is often exceeded.  The worst-case being queue 3 exceeding
  the threshold by 2 on the right-hand side of the plot. This problem
  can be attributed to the reaction time of the FIQ routine, and to
  the fact that jumping to the FIQ handler itself might cause a few
  memory transactions depending on the cache state. Currently, the
  thresholds used for FIQ-based regulation require to be fine-tuned
  manually by the user. Future extensions of the \schim will explore
  the implementation of schedulers capable of dynamically adapting the
  thresholds to maximize performance and improve isolation.

  The loss in bandwidth caused by routing transactions through the PL
  is important and a serious drawback against the adoption of the
  \schim.  The authors impute this drop of bandwidth to the path
  between the PS-side modules on the path from the core cluster to the
  HPM ports rather than on the frequency of the PL side itself. In
  fact, if one assumes a PL frequency of 250MHz and a single-burst
  beat of 128~bits per clock cycle, any bus segment implemented in the
  PL should be able to sustain a throughput of approximately
  3.7GBps. This is in line with the throughput reported
  in~\cite{uiuc-xilinx-port-study}. Moreover, it is important to
  remark that the difference in clock speed between CPUs and PL is
  misleading. This is because transactions originated by the CPUs
  cross a PS-side bus that operates at a much lower
  frequency. Specifically, on our platform the main bus subsystem
  operates at 500MHz whereas the CPUs operate at 1.5GHz. On the other
  hand, while the \schim does introduce some extra latency and thus
  has some impact on the available bandwidth, we can conclude that
  bulk of the penalty arises from the two levels of interconnects
  (CCI+Main Interconnect, see Figure~\ref{fig:PS-PL-diagram}) and
  FIFOs located on the path from the core cluster to the HPM
  ports. Therefore, it constitutes a limitation of the employed
  platform rather than of the proposed design.
}
