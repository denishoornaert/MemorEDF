\section{Introduction}

It is undeniable that the massive increase in expectation on the
performance of next-generation cyber-physical systems has deeply
impacted the way we design modern embedded and real-time
systems. High-resolution, high-bandwidth sensors such as lidars, and
depth cameras on the one hand, and data-intensive processing workload
such as machine-learning applications on the other hand, have
exacerbated the push for high-performance embedded
platforms. Following this performance \emph{moving target}, chip
manufactures have significantly scaled up clock speeds, CPU count, and
heterogeneity. For instance, the on-chip integration of powerful
graphic processing units (GPUs) has been the characterizing factor in
the NVIDIA Tegra series of embedded systems-on-a-chip (SoC).

In this context, an embedded architectural paradigm that is surging in
popularity among manufacturers, researchers, and industry
practitioners is the PS-PL organization. This class of embedded
platforms integrates on the same die (1) traditional full-speed
embedded CPUs and (2) programmable logic constructed using
field-programmable gate array (FPGA) technology. This organization
naturally defines two macro-domains, namely the Processing System (PS)
and the Programmable Logic (PL), hence the name. PS-PL platforms
establish a good trade-off between specialization, raw performance,
and mission-specific re-configurability. The current generation of
commercially available PS-PL platforms is dominated by ARM-based
products offered by, most notably, Intel~\cite{intel_stratix10} and
Xilinx~\cite{zynq_ultrascale}. A pilot large-scale, high-performance PS-PL
system is the Enzian platform~\cite{enzian2020cidr} being rolled out by ETH
Zurich\footnote{Also see \url{http://enzian.systems/}}. Furthermore, a
RISC-V-based solution has been recently made available by Microsemi
with their PolarFire SoC~\cite{microsemi_polarfire}.

From a real-time perspective, the co-existence of traditional CPUs and
a tightly-coupled block of PL has more profound implications than
expected. Clearly, it is possible to define custom accelerators in PL
and to relieve the main CPUs of some of the heavy data-processing
workload. However, more interestingly, recent studies have highlighted the
possibility of using the PL also as a way to manage the memory traffic
originated from the main CPUs~\cite{lime_2018, PLIM20}. Such a
possibility opens the doors for memory traffic inspection and control
at the level of individual transactions; which in turn promises to
unlock provable determinism for the real-time workload.

In this paper, we embrace the concept of PL-aided memory traffic
management and propose an infrastructure to develop, test and evaluate
memory scheduling policies. Specifically, we propose a component,
called the \schimL---or \schim, for short---that can be instantiated
in the PL to enforce a set of configurable scheduling policies on
individual memory transactions generated by the CPUs in the PS.

The overarching goal of the proposed \schim is twofold. First, we want
to provide a playground for researches to test promising novel memory
scheduling ideas for multi-core platforms, much like
LITMUS$^{\text{RT}}$~\cite{litmus-rt} fostered research on CPU
scheduling techniques. Second, we want our \schim to act as an
intermediate stepping stone for industrial applications where strong
determinism over memory performance is required. The \schim can be
used to analyze the behavior of realistic workload in a multitude of
what-if memory management use-cases. We note that such kind of
analysis was previously possible only through full-system simulation
or by synthesizing the entire SoC on FPGA---that is, with a soft-core
implementation.

In short, this paper makes the following contributions. (1) We
demonstrate that a configurable module could be interposed between the
cores and the memory controller to perform transaction-level
scheduling in commercial PS-PL platforms; (2) we propose a design for
a memory scheduling infrastructure that focuses on extensibility and
runtime reconfigurability; (3) we address important issues to
correctly account and regulate CPU-generated traffic when a shared
last-level cache is present; (4) we design and implement \replace{three}{two} pilot
memory scheduling policies as a proof-of-concept on the potential of
our \schim; and (5) we perform a full system integration and
implementation on a commercial PS-PL embedded platform to evaluate the behavior of the \schim with synthetic and realistic workload.


%% 6) We implement and evaluate a full-stack design that includes the
%% memory scheduler hardware module. Using this implementation, we
%% analyze several well-known scheduling algorithms, namely, Earliest
%% Deadline First (EDF), Least Laxity First (LLF), Time-division
%% multipleaccess (TDMA).



%% One of the key limiting factors for the adoption of modern multi-core
%% embedded platforms in safety-critical systems is complexity. As this
%% class of platforms increase in heterogeneity
