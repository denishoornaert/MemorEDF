\section{\schim Design and Implementation}
\label{sec:schim_implmentation}

A full-system implementation was carried out on a Xilinx ZCU102
development system, which is based on a Xilinx Zynq UltraScale+
XCZU9EG PS-PL SoC. The PS comprises four ARM Cortex-A53 CPUs
that share a unified 1~MB LLC. The PS includes a DDR4-2666 controller
connected to a 4~GB DDR4 memory module. There are two high-performance
master interfaces (HPM1 and HPM2); and a third interface routed
through the low power domain (LPM). The PL is capable of driving up to
16 interrupt requests lines towards the PS interrupt controller. We
hereby provide key details on the operation of our \schim in the
target platform. These include complementary software stack, memory
traffic accounting, regulation to prevent head-of-line blocking, and
programming model.

%% interacts with the remaining of the systems in
%% \ref{subsec:communication-scheme}, how its internal logic enforces
%% transaction scheduling policies in \ref{subsec:micro-arch} and
%% finally, an example of the transaction life cycle within the SchIM
%% module is provided in \ref{subsec:transaction-life-cycle}.

\subsection{Software Stack}
As mentioned in Section~\ref{sec:design_goals}, we want to ensure that
the \schim can be used with no modification to the OS and the
applications under analysis. For this reason, we rely on a thin
virtualization layer that can be used to redirect memory traffic from
the direct route to the loop-back route (see
Section~\ref{sec:bg_plim}). For this purpose, we use the open-source
Jailhouse~\cite{jailhouse} partitioning hypervisor\footnote{The source
  code is available at
  \url{https://github.com/siemens/jailhouse.git}.}  Jailhouse does not
boot the target machine. Instead, it relies on a standard Linux kernel
to perform the initial boot sequence. When enabled from a Linux
driver, Jailhouse dynamically virtualizes the original OS. In line
with its partitioning-only philosophy, Jailhouse has a small footprint
and enforces virtualization-aided partitioning of essential resources
like CPUs, interrupts, main memory, I/O devices. It does not perform
any virtual-CPU scheduling.

Following Jailhouse's nomenclature, a resource partition is called a
\emph{cell}, while guest OS's are referred to as \emph{inmates}. An
inmate can be either a bare-metal application, an RTOS or a
full-fledged OS like Linux. Jailhouse uses ARM hardware Virtualization
Extensions (VE) to offer a set of Intermediate Physical Address (IPA)
to its inmates that is compatible with the way they have been
compiled. Jailhouse then maps IPA ranges of different cells to
configurable Physical Addresses (PAs) --- stage-2 translation. By
changing the configured stage-2 mapping, it is possible to dynamically
re-route via the loop-back the memory traffic generated by each
inmate.

As described below, some modifications were necessary to the mainline
Jailhouse code for our full system implementation\footnote{The
  modified Jailhouse sources are available at \url{https://github.com/rntmancuso/jailhouse-rt}.}.

%% with a configurable color specification. By doing so, Jailhouse
%% supports the notion of temporal partitioning wherein, by assigning a
%% collection of non-overlapping sets of processors to colored inmates,
%% those cells cannot access a (physical) address beyond their own
%% address space resulting in interference prevention.

%% Jailhouse gets enabled on top of a Linux, called root-cell, and uses a
%% configuration file associated with it to split off parts of the
%% system's resources and assign them to desired cells. Last but not
%% least, Jailhouse is especially useful since one can circumvent the
%% TrustZone switches in any ARM-adapted OS (e.g., Linux), exploiting the
%% hypervisor calls.

\subsection{Altered communication scheme}\label{sec:communication-scheme}
In order to achieve the objective of re-ordering transactions, one
must alter the standard AXI communication scheme explained in the
Section~\ref{subsec:axi_transaction_scheme}.  To this end, the \schim
is interposed between the master (HPM) and the slave (HPS) as depicted
in \fig{fig:SchIM_transaction_scheme_figure}. As shown in
\fig{fig:SchIM_transaction_scheme_figure}, only the phases initiated
by the masters (i.e., address phase on AW and AR and the data phase on
W) are intercepted for re-ordering by the \schim.  The introduction of
the \schim has a direct consequence on the overall communication
scheme. Unlike the response phases on channels R and B that remain
unchanged, the address and write data phases are handled following a
store-and-forward scheme.  Consequently, a write transaction will
start exactly as in the standard AXI scheme with its address phase
\circled{1} and data phase \circled{2}.  These two transactions are
buffered within the \schim's queues (\circled{3}) and only relayed
following the internal memory scheduler's logic.  This release of the
transaction leads to the initialization of two new addresses and data
phase \circled{4}, and \circled{5}.  Finally, the response phase
\circled{6} goes directly from the slave to the master without being
intercepted.  For read transactions, the same modifications apply to
the address phase \circled{1'} which is buffered (\circled{2'}) for
some time before being re-emitted in \circled{3'}.  Just like for
write acknowledgments writing, the read response phase \circled{4'} is
not intercepted by the \schim.

%\begin{figure}
%  \centering
%  \input{tikz/SchIM_transaction_scheme.tex}
%  \caption{AXI communication with mediated by the \schim.}
%  \label{fig:SchIM_transaction_scheme_figure}
%\end{figure}

\subsection{Queueing Domain}
At the heart of the queueing domain, lies the queues. They work as FIFOs.
However, instead of inserting the new data at the back of the queue,
the new data is always inserted as close as possible to the front of the
queue. This mechanism helps avoiding gaps within the queues prevents the
loss of few clock cycles that would be required to move the data from
the back to the front. From the authors' experiments, saving clock cycles
in \schim is vital to keep the final bandwidth as high as possible.

Furthermore, the queues have been designed to deal with three
constraints.  Firstly, the queues store both read and write packets
such that the order at which transactions arrived is guaranteed. This
implies that all the queue slots have the same size regardless of
whether they contain read or write packets.  Secondly, due to the
altered communication scheme (see
Section~\ref{sec:communication-scheme}), each slot needs to be large
enough to store both the address phase payload and the corresponding
data of an AXI write transaction (678~bits). The depth of each queue
is determined by considering the worst-case scenario. The latter
consists of having to handle the maximum number of
outstanding read and write transactions simultaneously. Our \schim instance on the
considered Xilinx UltraScale+ platform was configured with queues that
are 16 slots in-depth. Indeed, the HPM ports in this platform cannot
handle more than eight transactions of each
type~\cite{Xilinx-ULTRASCALE-TRM}.

\subsection{LLC-\schim Interface and Traffic Accounting}
\label{sec:traffic-accounting}
As illustrated in \fig{fig:PS-PL-diagram}, the considered system
features an LLC shared between the four cores of the PS. For a
non-cacheable read (resp., write) memory access, which CPU represents
the source of the traffic is carried in the ID bits of the
corresponding AR (resp., AW) AXI transaction. But for cacheable memory
accesses, which is the norm for application workload, this is not the
case. This is mainly because cache controllers typically use a
write-back strategy. In this case, a read or write cache miss causes
up to two events: (1) a cache refill and (2) a cache eviction. The
cache refill is carried out with a read AXI transaction. If the line
being evicted was previously written (dirty), then the eviction causes
a write AXI transaction. It follows that, while read AXI transactions
have an easily identifiable source, write transactions do not. Indeed,
a CPU $x$ might be causing the eviction of a line previously allocated
and modified by CPU $y$. Hence, accounting (and scheduling) the
resulting write transaction as if it originated from CPU $x$ would be
incorrect.

To ensure fair accounting for both read and write traffic, we rely on
cache partitioning through coloring. As studied in a number of
previous works, cache coloring is easy to implement at the hypervisor
level~\cite{xvisor2018, cachepart, determ_virt}. In our system setup,
we leverage the support Jailhouse already provides. The standard
support has been extended to support booting a Linux inmate over
colored memory. Cache partitioning allows us to establish a 1-to-1
relationship between any read/write transaction traversing the \schim
and the originating CPU. Moreover, with cache coloring in place, the
\schim uses the color bits in the address of the memory transactions
(AR and AW channels) --- instead of the AXI ID bits --- to
differentiate between the traffic of the various cores.

Finally, recall that the \schim forwards transactions between HPM and
HPS ports. These ports follow the asynchronous AXI protocol that
allows issuing multiple outstanding AR and AW transactions. The
protocol dictates that any outstanding transaction must have a unique
AXI ID. This property is crucial to be able to match received
responses with outstanding requests. Unfortunately,
a potential mismatch between the bit-width of the AXI ID emitted at the HPM
ports and the bit-width of AXI ID accepted by the HPS ports. For
instance, in the platform of reference, the HPMs emit 16-bit AXI IDs,
while the HPS AXI ID bit-width is 6 bits. Therefore, the \schim also
acts as an AXI ID translator.

%% In the considered platform, the AXI ID is emitted by the LLC
%% controller inside the multi-core ARM Cortex-A53
%% cluster~\cite{ARM-cortex-A53}. The only master for write and read
%% transactions are \verb|0b1xxxnn| and \verb|0b1xxxx|,
%% respectively. Here, \verb|x| are variable values and \verb|nn| encode
%% the core ID. \todo[inline]{RM: incomplete! Please fix with actual IDs}

%% When a cache miss occurs, the LLC controller issues access the
%% targeted memory region by emitting an AXI transaction. The exact size
%% of the transaction (i.e., the amount of burst) will vary depending on
%% the size of the cache lines and the width of the bus. The LLC acting
%% as an intermediate and independent module, it emits transactions that
%% are not directly related to a specific core. For instance, in the case
%% of a cache line eviction, the decision of which line to evict is done
%% by the LLC controller alone.


\subsection{Scheduling Interface and Implemented Policies}\label{sec:sched_interf}
All the memory schedulers included in the scheduling domain share a
common interface to ease the integration of a new scheduler. In terms
of input signals, a generic scheduler module must define (1) a manual
reset signal that can be triggered through the configuration port; (2)
a vector of bits where each bit indicates whether the associated queue
is empty; and (3) a signal indicating if the last scheduled
transaction as been consumed. Alongside these inputs, the scheduling
modules also have access to all the configuration registers listed in
Table~\ref{tab:configuration_port_structure}. In terms of outputs a
\schim scheduler must define (1) a signal to the selector indicating
the queue considered for scheduling; and (2) a signal stating whether
the current scheduling decision is valid. We hereby review the initial
set of memory scheduling policies implemented in the \schim.

\subsubsection{Fixed Priority}\label{sec:fixed_prio}
The FP scheduling module aims at enforcing strict prioritization of
cores' memory traffic. The priority ordering is explicitly defined by
the user through the configuration port. While the \schim
\new{instance used in this paper} only has four queues, 16 different
levels of priority are offered \new{as the considered platform
  supports up to 16 different colors. This is useful if an hypervisor
  that supports vCPU scheduling is used. In this case, the \schim
  allows assigning different priorities to different partitions
  sharing the same physical CPU}. The core-to-priority assignment must
be strict, meaning that two cores cannot be assigned the same
priority.

The FP scheduling module only needs two pieces of information. That is
(1) the priority associated with each queue and (2) whether a given
queue contains at least one buffered transaction. The module logic
always selects the queue with the highest priority. Lower priority
queues are considered when higher priority queues do not have
transactions. This is done by internally setting the user-defined
priority of a queue as 0 when the corresponding queue is empty.

\subsubsection{Time Division Multiple Access}
The TDMA memory scheduler is a non-work conserving policy that
operates by defining a per-core time \emph{slot} during which the core
has exclusive access to main memory.  The slots are expressed in PL
clock cycles, to maximize granularity. The configuration port can be
used to specify and change the slots specifications at runtime.

The implementation of the module uses a counter register to track the
time elapsed in the current TDMA primary frame --- defined as the sum of
all the cores' slots. It is reset to 0 at the beginning of a new major
frame. Using the time-tracking register, the module determines to
which core the current slot belongs, and forwards the information to
the queue selector. This is done by summing up the length of all the
previous slots, and determining if the current time falls within the
interval of the considered core's slot.

%% Provided that the current value of the counting
%% register is contain between the sum of the previous periods and the
%% sum of the previous periods and the current one, the information
%% forwarded to the remaining of the system will be the core id
%% corresponding to the interval.  \todo[inline]{DH: @DH TODO improve
%%   description!}
\color{red}
\subsubsection{Traffic Shaping (TS)}
The proposed TS transaction scheduling policy operates by defining a
minimum inter-arrival time (MIT) that needs to elapse between any two
consecutive transactions from the same CPU. A transaction that arrives
before sufficient time has elapsed since the previous one is held by
the \schim until the aforementioned property is respected.

This scheduling module is implemented as follows. For each of the
\schim queues, the module defines a register counting the time elapsed
since the last forwarded transaction. Once this counter has reached
the period set by the user (through the configuration port), the
module checks if the queue corresponding to the core contains any
transaction. In the case where a transaction is available in the
corresponding queue, the latter is forwarded to the output of SchIM
(i.e., the serializer) and the counting register is reset to
0. Otherwise, the counting register is blocked to the desired period
until a transaction is available for scheduling in the corresponding
queue. Any tie between two cores is solved using a fixed priority
arbitration defined by the user.

The described TS policy is similar to the ARM QoS regulation mechanism
that is available at the level of interconnect for hardware
accelerators~\cite{qos-400, ewarp2020rtss}. It is also similar, at
least in principle, to software-based memory regulation techniques
such as MemGuard~\cite{memguard2013}. Yet, TS operates differently
from the hardware implementations of MemGuard-like regulation that
have been proposed so far in~\cite{zhou2016mitts,
  Farshchi2020BRUBR}. Indeed, our TS scheduler does not rely on memory
budgets and replenishment periods. Instead, it provides memory
bandwidth enforcement at the granularity of individual memory
transactions. This also differentiates the TS policy in the \schim
from the \emph{transaction supervisor} proposed
in~\cite{fred_hyperconnect}.
\color{black}

% TODO Move to appendix
\subsection{Programming Model}
The parameters that compose the programming interface of the \schim
are summarized in Table~\ref{tab:configuration_port_structure}. The
\texttt{base} address referenced in the table can be set when the
\schim is deployed in the PL. By default, this is set to
\texttt{0x800000000}. All the parameter registers are 32~bit wide,
except for the priorities of the FP scheduler. In this case, the
priority values are encoded using 8~bits. The last ``Mode'' register
allows a user to select the active memory scheduler.

\begin{table}[!ht]
  \centering
  \caption{Available \schim configuration registers.}
  \label{tab:configuration_port_structure}
  %\resizebox{0.8\columnwidth}{!}{%
\color{red}
  \begin{tabular}{|c||c|c|}
    \hline
    \multicolumn{1}{|c||}{Parameter}               & \multicolumn{1}{c|}{Associated Core}      & \multicolumn{1}{c|}{Address}        \\ \hline\hline
    \multirow{4}{*}{TDMA slots}             & $C_{0}$                                   & \texttt{base+0x00}                    \\ \cline{2-3}
    & $C_{1}$                                   & \texttt{base+0x04}                    \\ \cline{2-3}
    & $C_{2}$                                   & \texttt{base+0x08}                    \\ \cline{2-3}
    & $C_{3}$                                   & \texttt{base+0x0C}                    \\ \hline
    \multirow{4}{*}{User Thresholds}                & $C_{0}$                                   & \texttt{base+0x10}                    \\ \cline{2-3}
    & $C_{1}$                                   & \texttt{base+0x14}                    \\ \cline{2-3}
    & $C_{2}$                                   & \texttt{base+0x18}                    \\ \cline{2-3}
    & $C_{3}$                                   & \texttt{base+0x1C}                    \\ \hline
    FP Priorities                                & \begin{tabular}{c|c|c|c}
                                          $C_{0}$ & $C_{1}$ & $C_{2}$ & $C_{3}$\\
                                                \end{tabular}           & \texttt{base+0x20}                    \\ \hline
    \multirow{4}{*}{TS MITs} & $C_{0}$                                   & \texttt{base+0x24}                    \\ \cline{2-3}
    & $C_{1}$                                   & \texttt{base+0x28}                    \\ \cline{2-3}
    & $C_{2}$                                   & \texttt{base+0x2C}                    \\ \cline{2-3}
    & $C_{3}$                                   & \texttt{base+0x30}                    \\ \hline
    \multicolumn{3}{|c|}{Reserved}                                                                                              \\ \hline
    Mode                                      & N/A                                      & \texttt{base+0x38}                    \\ \hline
  \end{tabular}
\color{black}
\color{ForestGreen}
    \begin{tabular}{|c||c|c|}
      \hline
      \multicolumn{1}{|c||}{Parameter}               & \multicolumn{1}{c|}{Associated Core}      & \multicolumn{1}{c|}{Address}        \\ \hline\hline
      \multirow{4}{*}{TDMA slots}             & $C_{0}$                                   & \texttt{base+0x00}                    \\ \cline{2-3}
      & $C_{1}$                                   & \texttt{base+0x04}                    \\ \cline{2-3}
      & $C_{2}$                                   & \texttt{base+0x08}                    \\ \cline{2-3}
      & $C_{3}$                                   & \texttt{base+0x0C}                    \\ \hline
      \multirow{4}{*}{User Thresholds}                & $C_{0}$                                   & \texttt{base+0x10}                    \\ \cline{2-3}
      & $C_{1}$                                   & \texttt{base+0x14}                    \\ \cline{2-3}
      & $C_{2}$                                   & \texttt{base+0x18}                    \\ \cline{2-3}
      & $C_{3}$                                   & \texttt{base+0x1C}                    \\ \hline
      FP Priorities                                & \begin{tabular}{c|c|c|c}
                                            $C_{0}$ & $C_{1}$ & $C_{2}$ & $C_{3}$\\
                                                  \end{tabular}           & \texttt{base+0x20}                    \\ \hline
      \multicolumn{3}{|c|}{Reserved}                                                                                              \\ \hline
      Mode                                      & N/A                                      & \texttt{base+0x38}                    \\ \hline
    \end{tabular}%
    %}
\color{black}
\end{table}
