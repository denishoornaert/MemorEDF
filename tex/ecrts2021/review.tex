\newpage
\section{Replies to Reviews and Shepherding Instructions}

    \subsection{Shepherding Requirements}

        \subsubsection{Please remove the discussion of the TS
        scheduler since it has logical errors in its current status as
        highlighted in the paper and shown in evaluation}

         \label{subsubsec:ts_removal}
            Every mention of the TS scheduler have been removed.
            \begin{itemize}
                \item line 303-304, 312-313: mentions removed
                \item line 444-465: whole section 5.5.3 removed
                \item Table 1: TS entries removed from configuration port
                \item line 541: mentions removed
                \item Figure 4: TS bar cluster removed
                \item line 557-558, 563-564: mentions removed
                \item line 581-582: mentions removed
                \item Figure 5: TS inset removed
                \item line 601-602, 603, 604-605: mentions removed
                \item Figure 6: TS inset removed
                \item line 624-632: TS trace snapshot discussion removed
                \item line 651-652, 661-665: TS isolation discussion removed
                \item line 735: mentions removed
            \end{itemize}

        \subsubsection{Add a discussion about the limitations of the
        work as highlighted by the reviewers}

            A discussion regarding the limitations and the nuances of
            our approach compared to related works has been integrated
            in Section~\ref{sec:discussion}.

            The section is organized as follow:
            \begin{itemize}
                \item Paragraph 1: we discuss the reason for the \schim
                to be currently implemented as a centralized scheduler
                and mention the necessity of a more decentralized
                re-design in platforms with multiple memory
                controllers;

                \item Paragraph 2: we discuss the requirements of the \schim
                in terms of hypervisor;

                \item Paragraph 3: we make the necessary
                considerations about the PL-to-PS feedback and its
                implementation using FIQ lines.

                \item Paragraph 4: we discuss the bandwidth cost to
                pay for routing the traffic through the PL and discuss
                the reason of the loss in net bandwidth;
           \end{itemize}

        \subsubsection{Explain the value the proposed
        scheduler-in-the-middle is adding to the isolation
        use-case. In particular, address the following concern: you
        experiment with enforced DRAM bank partitioning, which is
        known by existing literature to enable such isolation on its
        own. Ideally, a comparison with the system, where
        scheduler-in-the-middle was applies without bank partitioning
        and the system where only bank partitioning was applied
        without the scheduler-in-the-middle will help distilling the
        effectiveness of each one.}

        During the implementation, we have noticed that
        work-conserving policies such as FP could not be enforced,
        because, without bank partitioning, the DRAM controller might
        partially re-order transactions to the same bank as it applies
        the FR-FCFS scheduling policy to per-bank queues. Using DRAM
        bank partitioning prevents the DRAM controller from reordering
        the ordering of transactions produced by the \schim.  In other
        words, we ensure that transactions will be served in the order
        dictated by \schim. While we agree that differentiating the
        degree of achieved isolation with and without bank
        partitioning would represent an interesting evaluation, we are
        unable to include additional experiments to cover this aspect
        due to time constraints. The reason why the \schim improves
        isolation even in the presence of bank partitioning is that
        contention at the DRAM banks is not the only source of
        contention. As many works have demonstrated in the context of
        memory bandwidth regulation, the shared memory controller
        itself with its shared command and data buses represents a
        source of contention. It is accepted that bank partitioning is
        required in addition to bandwidth regulation. The \schim
        brings memory bandwidth management into the PL, and provides
        not only regulation but a generic infrastructure to experiment
        with custom bandwidth management techniques, both
        work-conserving and non-work-conserving.

        %% Similarly, we could have redirected the
        %% whole traffic to one single DRAM bank. However, realizing our
        %% experiments on 128MB would have been difficult.  Henceforth,
        %% the \schim module can enforce the isolation as, from its
        %% centralized nature, it is the bus bottleneck.  That being
        %% said, we agree to perform the experiences suggested by the
        %% reviewers if the time frame allows us to do so.

    \subsection{Reviewer A}

        \subsubsection{Responses are not intercepted by SchIM,
        preventing response time analysis of different schedulers.}

            We thank the reviewer for raising this excellent
            point. Having a clearer view of the response time of
            different memory components at the transaction level is
            one of the many features that can be made available by the
            PLiM approach. While the authors are interested, and in
            many ways already invested, in exploring such modules in
            the future, we consider this as currently out of the scope
            of this specific work since our main focus for this first
            iteration of the \schim infrastructure is to enforce and
            test scheduling policies.

        \subsubsection{The TS scheduler has logic errors. It is
        unclear why it is still included in the paper.}

            Every element linked to the TS policy have been
            removed. See Section \ref{subsubsec:ts_removal} for more
            information.

    \subsection{Reviewer B}

        \subsubsection{Serialization through HPMs and the PLIM design with
        a single central memory scheduler severly limits the approach
        and constraints the kind of systems for which memory
        scheduling can be explored.}

        The authors acknowledge that this is a current limitation of
        our \schim. This matter as been addressed and discussed in
        Section~\ref{sec:discussion}.

        \subsubsection{Given that PLIM and PL-side memory arbitrarion
        existed before, the originality of the work is limited.}

        The \schim module leverages the key capability postulated in
        the original PLiM paper~\cite{PLIM20}. However, unlike the
        bleaching module described in~\cite{PLIM20}, the \schim aims
        to have an active behavior, meaning that the timing of memory
        transactions is explicitly managed thus providing the ability
        to enforce custom scheduling policies on memory traffic.
        Regarding the PL-side arbitration, the authors acknowledge
        that previous works (e.g., Hyperconnect from the FRED
        framework~\cite{fred_hyperconnect}) have explored the arbitration of concurrent
        transactions, as we also discuss in
        Section~\ref{sec:relwork}. However, these works are applicable
        to memory streams originating in the PL and cannot be directly
        applied to manage traffic originating from PS-side processing
        elements. This sets the \schim apart from that line of works
        as it introduces a complementary set of techniques. Following
        the shepherding requirements, this point has been emphasized
        in the third paragraph of Section~\ref{sec:design_goals}.

%        \subsubsection{It is not clear to me whether the evaluation correctly reflects the performance of the baseline.}


        \subsubsection{The paper could be written more conscisely. In
        particular the figures in the evaluation must be scaled up.}

        With the removal of the content and experiments related to the
        TS policy, the paper has been compressed. The extra room has
        been used to scale up the figures.

        \subsubsection{Taking away FIQ from the guest OS further
        impacts performance. Please explain why you did not use
        Monitor mode, in particular since you do not need to spill
        many registers for executing}

        This matter is discussed in the 3rd paragraph of the
        Section~\ref{sec:discussion}. The authors are unsure about
        what the reviewer refers to with the term "Monitor mode." We
        assume they are referring to the EL3 (Secure Monitor). We have
        added a brief discussion in Section~\ref{sec:discussion} that
        touches on the possibility of handling FIQs in EL3 instead of
        EL2. In short, when handling FIQs in EL3, there is an extra
        overhead to be paid because the CPU needs to switch to secure
        mode. Thus, while not practically impossible, we expect that
        handling FIQs in EL3 might incur higher overhead compared to
        our choice of handling FIQs in EL2.

        \subsubsection{Please explain why FP offers 16 priorities for
        4 queues.}

        We have added a short explanation in
        Section~\ref{sec:fixed_prio}. In short, our \schim offers up
        to 16 levels of priority because on the target platform 16
        cache colors are present. Thus, one can potentially define up
        to 16 partitions and, by extension, up to 16 queues. This
        makes sense if multiple partitions can share the same core if
        an hypervisor that supports vCPU scheduling is employed. This
        is currently not the case for the considered Jailhouse
        hypervisor.

    \subsection{Reviewer C}

        \subsubsection{The paper is excessively overselling the idea
        while underestimating the weaknesses.}

        We agree that there was a recognizable mismatch in term of
        claims between our introduction/motivation sections
        (specifically the third paragraph in
        Section~\ref{sec:design_goals}) and the rest of the technical
        content. Accordingly, we revised
        Section~\ref{sec:design_goals} to more consistently position
        the contribution of our work.

        \subsubsection{The paper does not solve in fact these
        problems, while introducing new ones. It does not solve these
        problems because still SchIM is implemented in PL which is
        running on order-of-magnitude slower clock frequency than the
        PS. This is clear from the results where SchIM kills the
        memory performance by more than 75\% throughput degradation
        (Figure 4).}

        Following the shepherding requirements, this point is
        discussed in Section~\ref{sec:discussion}. Briefly, we
        acknowledge that the \schim introduces an overhead. However,
        this overhead is limited once compared to the throughput
        required to just route memory traffic through the
        PL. Moreover, it appears that the quite high cost of
        redirecting the traffic through the PL is not induced by its
        frequency. In fact, a back-of-the envelope calculation reveals
        that for a PL operating at 250MHz (the \schim frequency), and
        with a bus width of 128~bits, a full-duplex throughput of
        approximately 3.7GBps can be sustained. This is in line with
        the throughput of 4.8GBps (for 300MHz) reported
        in~\cite{uiuc-xilinx-port-study}.  Finally, while the Cortex
        A53 cores run at 1.5GHz, the main PS-side bus runs at a much
        lower frequency, i.e. 500MHz. This frequency is closer to the
        one of the PL-side. Thus, we can conclude that a particularly
        inefficient implementation of the PS-PL interfaces available
        in the target platform is to blame for the observed drop in
        available bandwidth. The proposed \schim design transcends the
        carried out implementation and better raw performance are
        expected for the same design as PS-PL platforms proliferate
        and increase in maturity.

        \subsubsection{Given that the paper also addresses performance
        isolation and guarantees as a use case considering real-time
        systems as a potential customer, bounding memory latencies is
        also an important factor. There has been a plethora of
        research papers in the community to bound (and tighten) these
        latencies. This solution will definitely increase these
        latencies and that effect is not quantified or even
        highlighted in the paper.}

        Our concern in this sense resonates with the reviewer. Indeed,
        being able to more accurately control and bound the latency of
        memory transactions is one of the main premises of the \schim
        framework. We aim at offering the possibility to control the
        latency and the timings at a transaction level with
        user-configurable and community-enhanced memory policies.  The
        two policies offered in this paper serve the sole purpose of
        demonstrating that the \schim, as a framework, is capable of
        scheduling cpu-originated traffic. Implementing policies that
        could guarantee a user-specified latency on a per-transaction
        basis is part of the broad range of future works made possible
        by the \schim.  While interesting, we consider such
        contributions worthy of self-contained investigation.

%       \paragraph{Those schedulers are implemented on top of (or on
%       the way to) the actual memory controller existing schedulers
%       (such as FR-FCFS). Accordingly, they do not cancel its
%       effect.}  \paragraph{the system designer still needs to apply
%       the existing techniques from literature to bound the
%       interference of these memory controller schedulers.}

        \subsubsection{With regard to the PL-to-PS feedback: It is not
        clear to the reviewer how the effectiveness of this is
        different from the natural backpressure/backlogging mechanisms
        deployed by most existing COTS SoCs?}

        The motivation behind the integration of the PL-to-PS feedback
        is that the traditional backpressure mechanisms cannot
        differentiate and/or prioritize between the cores as, from the
        point of view of the memory subsystem, they appear as one
        master (i.e., the core cluster or LLC controller).  Thus,
        while the FIFOs located before the HPM ports can stop the
        arrival of new transactions in order to not overflow, they
        cannot indicate which specific core should be throttled. On
        the other hand, \schim can throttle a specific core even if
        the regulation is coarse.  The throttling is essential to
        ensure that cores are assigned a priority-aware share of the
        memory bandwidth as it prevents head-of-the-line blocking. We
        provide a discussion on the topic in
        Section~\ref{sec:pl-to-ps-feedback}. One practical example is
        the following. As shown by Valsan et al in ``Taming
        Non-Blocking Caches to Improve Isolation in Multicore
        Real-Time Systems'' (RTAS'16) when due to the combined
        activity of all the cores the available MSHR are exhausted,
        the entire LLC can block. The PL-PS feedback mechanism
        introduced with the \schim is a congestion control mechanism
        that can target individual cores for throttling preventing the
        entire cluster to stall due. Because throttling is
        priority-aware, it ensures that high-priority cores are
        shielded from low priority cores that emit high-rate memory
        transactions.

        %% Finally, we think that MSHRs regulation is not helpful in this
        %% situation for two reasons: \begin{enumerate} \item The total
        %% amount of outstanding transactions allowed by the MSHRs is
        %% greater than the total capacity offered by the two FIFOs
        %% located before the HPM ports. For instance, on our board, the
        %% LLC can accommodate up to 64 outstanding read transactions,
        %% whereas, together, the FIFOs can only accommodate up to 16
        %% outstanding read transactions. Hence, MSHR regulation alone
        %% cannot prevent head-of-the-line blocking.  \item As hinted in
        %% section \ref{sec:pl-to-ps-feedback}, while the amount of read
        %% transactions emitted by a given core can be bounded thanks to
        %% an MSHR counter, this is not the case for write
        %% transactions. In fact, the LLC write-back unit is shared
        %% without any restrictions amongst the cores as a write-back
        %% request result of the eviction of a cache line. In such a
        %% mechanism, the ownership of the cache line is lost. Similar to
        %% the previous point, this lack of regulation can lead to
        %% head-of-the-line blocking in the HPM ports
        %% FIFOs.  \end{enumerate}

        \subsubsection{With regard to the PL-to-PS feedback: Compared
        to the aforementioned already implemented techniques, this
        PL-to-PS feedback has its own drawbacks:}

        The drawbacks mentioned by the reviewers are (i) the latency
        inherent to the interruption caused by the PL-to-PS feedback
        and (ii) the coarseness of the feedback.  The authors
        acknowledge the aforementioned drawbacks and discuss them in
        Section~\ref{sec:discussion}. In both cases, the problem
        resides in the amount of time required to invoke the FIQ
        routine. Bounding this latency and understanding how often the
        threshold is exceeded is a non-trivial task. This latency can
        however be shortened and made more predictable in a couple of
        ways. For instance, one could allocate the instructions (no
        data are used inside the handler) that correspond to the FIQ
        handler in a private cache color. This would prevent extra
        memory transactions to ever being generated when the FIQ is
        invoked due to cache misses on the FIQ handler
        instructions. Alternatively, the FIQ instructions could be
        allocated in a block of tightly coupled memory (e.g. a
        scratchpad) and marked as non-cacheable.
