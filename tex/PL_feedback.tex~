\section{PL-to-PS Feedback}
    \todo[inline]{DH: Here, we need to argue why we need a PL feedback (i.e., Queues before the HPM ports), how it can impact SchIM (i.e., nothing can be applied since the queues can be saturated by transaction coming from other cores), how we solved it (i.e., comparators in each queue, comparing the current amount of buffered transactions with a threshold defined by the user through the configuration port, sending an interrupt at the same clock cycle to the PS side, which considers this interrupt line as an FIQ. The FIQ is handled by a routine in the hypervisor that set a DBS instruction (or Data Barrier Synchronisation) in order to kill/stop the core until all the pending transaction have been served.) and how, intuitively, we can choose the values for each proposed Scheduling policies.}
    
    While the combination of the PS and the PL sides offer great opportunities, it also comes with challenges. In fact, each of the HPM ports interfacing the PS and the PL sides (HPM0 and HPM1) have two dedicated queues for read and write transactions. From ??, we know that they all have a depth of 8. Since transactions are being buffered inside SchIm as well as in these port buffers, head-of-the-line blocking can happen. Head-of-the-line blocking can be armful for performance or simply cancel all the efforts put in place to enforce transaction reordering and core isolation. For instance, in the case of a non work-conserving policy (e.g., TDMA), if the HPM port queue gets filled with transaction coming for the same core, 
