\section{\schim Design and Implementation}
\label{sec:schim_implmentation}

The present section provides important details that are crucial for
the operation of our \schim. These include memory traffic accounting,
regulation to prevent head-of-line blocking, and programming models.

%% interacts with the remaining of the systems in
%% \ref{subsec:communication-scheme}, how its internal logic enforces
%% transaction scheduling policies in \ref{subsec:micro-arch} and
%% finally, an example of the transaction life cycle within the SchIM
%% module is provided in \ref{subsec:transaction-life-cycle}.

\subsection{Software Stack}
As mentioned in Section~\ref{sec:design_goals}, we want to ensure that
the \schim can be used with no modification to the OS and the
applications under analysis. For this reason, we rely on a thin
virtualization layer that can be used to redirect memory traffic from
the direct route to the loop-back route (see
Section~\ref{sec:bg_plim}). For this purpose we use the open-source
Jailhouse~\cite{jailhouse} partitioning hypervisor\footnote{The source
  code is available at
  \url{https://github.com/siemens/jailhouse.git}.}  Jailhouse does not
boot the target machine. Instead, it relies on a standard Linux kernel
to perform the initial boot sequence. When enabled form a Linux
driver, Jailhouse dynamically virtualizes the original OS. In line
with its partitioning-only philosophy, Jailhouse has a small footprint
and enforces virtualization-aided partitioning of essential resources
like CPUs, interrupts, main memory, I/O devices. It does not perform
any virtual-CPU scheduling.

Following Jailhouse's nomenclature, a resource partition is called a
\emph{cell}, while guest OS's are referred to as \emph{inmates}. An
inmate can be either a bare-metal application, an RTOS or a
full-fledged OS like Linux. Jailhouse uses ARM hardware Virtualization
Extensions (VE) to offer a set of Intermediate Physical Address (IPA)
to its inmates that is compatible with the way they have been
compiled. Jailhouse then maps IPA ranges of different cells to
configurable Physical Addresses (PAs) --- stage-2 translation. By
changing the configured stage-2 mapping, it is possible to dynamically
re-route via the loop-back the memory traffic generated by each
inmate. 

%% with a configurable color specification. By doing so, Jailhouse
%% supports the notion of temporal partitioning wherein, by assigning a
%% collection of non-overlapping sets of processors to colored inmates,
%% those cells cannot access a (physical) address beyond their own
%% address space resulting in interference prevention.

%% Jailhouse gets enabled on top of a Linux, called root-cell, and uses a
%% configuration file associated with it to split off parts of the
%% system's resources and assign them to desired cells. Last but not
%% least, Jailhouse is especially useful since one can circumvent the
%% TrustZone switches in any ARM-adapted OS (e.g., Linux), exploiting the
%% hypervisor calls.

\subsection{LLC-\schim Interface and Traffic Accounting}
As illustrated in Fig. \ref{fig:PS-PL-diagram}, the considered system
features an LLC shared between the four cores of the PS. For a
non-cacheable read (resp., write) memory access, which CPU represents
the source of the traffic is carried in the ID bits of the
corresponding AR (resp., AW) AXI transaction. But for cacheable memory
accesses, which is the norm for application workload, this is not the
case. This is mainly because cache controllers typically use a
write-back strategy. In this case, a read or write cache miss causes
up to two events: (1) a cache refill and (2) a cache eviction. The
cache refill is carried out with a read AXI transaction. If the line
being evicted was previously written (dirty), then the eviction causes
a write AXI transaction. It follows that, while read AXI transaction
have an easily identifiable source, write transactions do not. Indeed,
a CPU $x$ might be causing the eviction of a line previously allocated
and modified by CPU $y$. Hence, accounting (and scheduling) the
resulting write transaction as if it originated from CPU $x$ would be
incorrect.

In consequence, the emitted transactions do not carry the information
about the requesting cores as the LLC controller is the only
master. On ARM Cortex-A53 \ref{ARM-cortex-A53}, write transactions
(AWID) and read transactions (ARID) are respectivelly tagged with IDs
\verb|0b1xxxx| and \verb|0b1xxxnn| where \verb|xxxx| is any number and
\verb|nn| is the core ID.

When a cache miss occurs, the LLC controller issues access the
targeted memory region by emitting an AXI transaction. The exact size
of the transaction (i.e., the amount of burst) will vary depending on
the size of the cache lines and the width of the bus. The LLC acting
as an intermediate and independent module, it emits transactions that
are not directly related to a specific core. For instance, in the case
of a cache line eviction, the decision of which line to evict is done
by the LLC controller alone.


\subsection{Altered communication scheme}
\label{subsec:communication-scheme}

In order to achieve the objective of re-ordering transactions, one
must alter the standard AXI communication scheme explained in the
subsection \ref{subsec:axi_transaction_scheme}.  To this end, we
instantiate a in-between the master and the slave a pass-through
module named SchIM as depicted in figure
\ref{fig:SchIM_transaction_scheme_figure}.  This module is the one in
charge of the re-ordering of the transactions and does so by
intercepting on the fly the transactions emitted by the masters before
they reach the desired slaves.  As shown in the figure
\ref{fig:SchIM_transaction_scheme_figure}, only the phases
instantiated by the masters (i.e. address phase on AW and AR and the
data phase on W) are intercepted for re-ordering by SchIM.  The
introduction of SchIM has a direct consequence on the overall
communication scheme. Indeed, while the response phases on channels R
and B remain unchanged, the address and data phases are duplicated.
Consequently, a write transaction will start exactly as in the
standard AXI scheme with its address phase \circled{1} and data phase
\circled{2}.  These two transactions are buffered within the SchIm
module in \circled{3} and only repeated when decided by the latter's
internal logic.  This release of the transaction leads to the
initialisation of two new address and data phase \circled{4} and
\circled{5}.  Finally, the response phase \circled{6} goes directly
from the slave to the master without being intercepted.  The same
modifications apply to the transmissions of read transactions as the
address phase \circled{1'} is being buffered in \circled{2'} for some
time before being re-emitted in \circled{3'}.  As for the writing, the
response phase \circled{4'} is not intercepted by SchIM.

\begin{figure}
  \centering
  \input{tikz/SchIM_transaction_scheme.tex}
  \caption{Caption}
  \label{fig:SchIM_transaction_scheme_figure}
\end{figure}

\subsection{Configuring and reprogramming SchIM}
\begin{table}[!ht]
  \centering
  \caption{Available configuration registers for SchIM}
  \label{tab:configuration_port_structure}
  \begin{tabular}{|c||c|c|}
    \hline
    \multicolumn{1}{|c||}{Field}               & \multicolumn{1}{c|}{Associated Core}      & \multicolumn{1}{c|}{Address}        \\ \hline\hline
    \multirow{4}{*}{TDMA periods}             & $C_{0}$                                   & \verb|base+0x00|                    \\ \cline{2-3} 
    & $C_{1}$                                   & \verb|base+0x04|                    \\ \cline{2-3} 
    & $C_{2}$                                   & \verb|base+0x08|                    \\ \cline{2-3} 
    & $C_{3}$                                   & \verb|base+0x0C|                    \\ \hline
    \multirow{4}{*}{Threshold}                & $C_{0}$                                   & \verb|base+0x10|                    \\ \cline{2-3} 
    & $C_{1}$                                   & \verb|base+0x14|                    \\ \cline{2-3} 
    & $C_{2}$                                   & \verb|base+0x18|                    \\ \cline{2-3} 
    & $C_{3}$                                   & \verb|base+0x1C|                    \\ \hline
    FP Priorities                                & $\{C_{0}, C_{1}, C_{2}, C_{3}\}$          & \verb|base+0x20|                    \\ \hline
    \multirow{4}{*}{MG inter-arrival periods} & $C_{0}$                                   & \verb|base+0x24|                    \\ \cline{2-3} 
    & $C_{1}$                                   & \verb|base+0x28|                    \\ \cline{2-3} 
    & $C_{2}$                                   & \verb|base+0x2C|                    \\ \cline{2-3} 
    & $C_{3}$                                   & \verb|base+0x30|                    \\ \hline
    \multicolumn{3}{|c|}{Reserved}                                                                                              \\ \hline
    Mode                                      & N.A.                                      & \verb|base+0x38|                    \\ \hline
  \end{tabular}
\end{table}


\subsection{Embedded Schedulers}
All the embedded schedulers included in the scheduling domain share a
common interface, making the integration of a new scheduler
seamless. As inputs, a scheduler module has a manual reset signal that
can be triggered through the configuration port, a vector of bits each
indicating whether the associated queue is empty and a signal
indicating if the latest transaction scheduled as been
consumed. Alongside these inputs, the scheduling modules also have
access to all the configuration registers listed in Table
\ref{tab:configuration_port_structure}. The module has for output the
ID of the queue considered for scheduling and a signal stating whether
the ID is valid.

\subsubsection{Fixed Priority}
The Fixed Priority scheduling module aims at enforcing prioritization
of the traffic of the cores. The priority ordering is explicitly
defined by the user through the configuration port. While SchIM only
has four queues, 16 different levels of priority are offered. Note
that, as it stands, the ordering is strict, meaning that two cores
cannot be assigned with the same priority.

The FP scheduling module only needs two information: the priority
associated with each queue and whether a given queue contains at least
one buffered transaction. Intuitively, the module logic consists in
always considering the highest priority hence, a argmax function is
implemented. However, lower priority queues must also be considered
when higher priority queue do not have transactions. Consequently, the
user defined priority is temporarily considered as 0 when its
corresponding queue does not contain transaction.

\subsubsection{Time Division Multiple Access}
The Time Division Multiple Access (or TDMA) scheduling module provided
by SchIM is a non-work conserving policy that splits the bus usage
between the cores for given periods (also referred to as slots). The
proposed embedded TDMA module enables the user to specify a different
TDMA slot size for each core. The periods are expressed in clock
cycles, enabling a fine grained granularity. The configuration port
enables the end-user to specify and change the periods at runtime.

The implementation of the modules relies on a single register used as
a counter. In fact, we use this register to count the time elapsed in
the current TDMA hyper-period (i.e., the sum of all the cores period)
and is reset to 0 once this hyper-period reached. Alongside this
register, a logic is there to determine in which core slot the counter
actually is and to forward the information to the queue selector. The
logic is able to determine the core to schedule by summing the period
of each previous cores. Provided that the current value of the
counting register is contain between the sum of the previous periods
and the sum of the previous periods and the current one, the
information forwarded to the remaining of the system will be the core
id corresponding to the interval.  \todo[inline]{DH: @DH TODO improve
  description!}

\subsubsection{MemGuard}
The proposed MemGuard transaction scheduling policy is inspired by the
Software-based bus regulation technique ??. The latter has been proved
to ensure memory isolation for all the cores involved. Unlike the
original MemGuard, the proposed version does not rely on memory
budgets and replenishment periods. Instead, it enforces inter-arrival
time (i.e. a guaranteed minimal period) between two consecutive
transactions emitted by a given core. This distinguishes our approach
from \cite{Farshchi2020BRUBR}.

This scheduling module is implemented as follows. For each of the
queues to schedule, the module has a register counting the time
elapsed since the start of the period. Once this counter has reached
the period set by the user (through the configuration port), the
module checks if the queue corresponding to the core contains any
transaction. In the case where a transaction is available in the
corresponding queue, the latter is forwarded to the output of SchIM
(i.e., the serializer) and the counting register is reset to
0. Otherwise, the counting register is blocked to the desired period
until a transaction is available for scheduling in the corresponding
queue, leading to the counting register to be reset to 0. Any tie
between two cores is solved using a fixed priority arbitration defined
by the user thanks to the configuration port.


\subsection{Transactions Life Cycle}
\label{subsec:transaction-life-cycle}

Let us consider a system with four cores $c_0, \ldots, c_3$ sending
transactions $T = \{t_{0}, t_{1}, ..., t_{n}\}$ to the \schim module.
Consequently, the latter boasts four queues (noted $Q = \{q_{0},
q_{1}, q_{2}, q_{3}\}$) buffering the transactions under the form of
packets $P = \{p_{0}, p_{1}, ..., p_{n}\}$ where $p_{i} =
Packetizer(t_{i})~\forall i \in [0 : n]$.

In the present example, we will assume $t_{1}$ as being the
transaction under analysis.  The latter is emitted by $c_{2}$ in
direction of the \schim module.  The packetizer receives this
transaction and, once the AXI protocol completed, transform it into an
equivalent packet $p_{1} = Packetizer(t_{1})$.  Following this
transformation, the newly created packet is forwarded to the
dispatcher which, thanks to the emitter's id embedded within the
transaction, is re-routed to the corresponding queue $q_{2}$ (since
emitted by $c_{2}$).  After the insertion of $p_{1}$ in $q_{2}$, the
state of the queuing domain is as follows: $q_{0}$ has two packets
$p_{0}$ and $p_{k}$ and $q_{2}$ only has $p_{1}$.  At this point,
$q_{0}$ is considered for scheduling by the scheduling domain.  In
consequence, $p_{0}$ is forwarded to the serializer through the
selector.  Simultaneously to the reception of the packet by the
serializer, the latter receives an activation signal from the
scheduling domain informing the serializer that the packet is valid
and that a transaction can be started.  Similarly to the packetizer,
the serializer will transform the packet $p_{0}$ back to its initial
AXI transaction form $t_{0} = Serializer(p_{0})$.  Thereafter, once
the $t_{0}$ has been sent, the serializer will inform the scheduling
domain via a signal, that he is ready to accept the next packet as
input.  Upon the reception of this signal, the scheduling domain will
both re-direct the latter to the queue of the previous packet to
indicate that it has been consumed and change the selected queue
according to the scheduling policy so that the first packet of this
queue can be forwarded to the serializer through the selector module.
In the present example, the "consumed" signal forwarded by the
scheduler is sent to $q_{0}$ which is then empty.  At this instant,
two scenarios are possible:

\begin{enumerate}
\item $q_{0}$ is still considered for scheduling following the
  selected scheduling policy. Therefore, as $q_{0}$ is empty, it
  outputs an "empty" signal received by the scheduling domain.  The
  latter then decides to not send any activation signal to the
  serializer because there is nothing left to transmit in the selected
  queue.  In other words, the access to the main memory is being
  stalled on purpose by the scheduling policy i.e. the scheduling
  policy is not work conserving.  For instance, such a scenario could
  happen in the case of TDMA or if all the queues are empty.  The
  logic will resume as soon as the selected queue is filled.
\item $q_{2}$ is now considered instead of $q_{0}$ for scheduling.  In
  this case, the "consumed" signal is repeated to $q_{0}$ while the
  queue ID changes in order to select $q_{2}$.  This results in the
  packet contained inside $q_{1}$ to be forwarded to the selector.
\end{enumerate}

