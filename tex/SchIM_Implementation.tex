\section{\schim Design and Implementation}
\label{sec:schim_implmentation}

The present section provides important details that are crucial for
the operation of our \schim. These include memory traffic accounting,
regulation to prevent head-of-line blocking, and programming models.

%% interacts with the remaining of the systems in
%% \ref{subsec:communication-scheme}, how its internal logic enforces
%% transaction scheduling policies in \ref{subsec:micro-arch} and
%% finally, an example of the transaction life cycle within the SchIM
%% module is provided in \ref{subsec:transaction-life-cycle}.

\subsection{Platform of Reference}
\todo[inline]{RM: Perhaps a good place to add a few words about the
  target platform.}

\subsection{Software Stack}
As mentioned in Section~\ref{sec:design_goals}, we want to ensure that
the \schim can be used with no modification to the OS and the
applications under analysis. For this reason, we rely on a thin
virtualization layer that can be used to redirect memory traffic from
the direct route to the loop-back route (see
Section~\ref{sec:bg_plim}). For this purpose we use the open-source
Jailhouse~\cite{jailhouse} partitioning hypervisor\footnote{The source
  code is available at
  \url{https://github.com/siemens/jailhouse.git}.}  Jailhouse does not
boot the target machine. Instead, it relies on a standard Linux kernel
to perform the initial boot sequence. When enabled form a Linux
driver, Jailhouse dynamically virtualizes the original OS. In line
with its partitioning-only philosophy, Jailhouse has a small footprint
and enforces virtualization-aided partitioning of essential resources
like CPUs, interrupts, main memory, I/O devices. It does not perform
any virtual-CPU scheduling.

Following Jailhouse's nomenclature, a resource partition is called a
\emph{cell}, while guest OS's are referred to as \emph{inmates}. An
inmate can be either a bare-metal application, an RTOS or a
full-fledged OS like Linux. Jailhouse uses ARM hardware Virtualization
Extensions (VE) to offer a set of Intermediate Physical Address (IPA)
to its inmates that is compatible with the way they have been
compiled. Jailhouse then maps IPA ranges of different cells to
configurable Physical Addresses (PAs) --- stage-2 translation. By
changing the configured stage-2 mapping, it is possible to dynamically
re-route via the loop-back the memory traffic generated by each
inmate. 

As described below, some modifications were necessary to the mainline
Jailhouse code for our full system implementation\footnote{The
  modified Jailhouse sources are available at \emph{URL OMITTED FOR
    BLIND REVIEW}.}.

%% with a configurable color specification. By doing so, Jailhouse
%% supports the notion of temporal partitioning wherein, by assigning a
%% collection of non-overlapping sets of processors to colored inmates,
%% those cells cannot access a (physical) address beyond their own
%% address space resulting in interference prevention.

%% Jailhouse gets enabled on top of a Linux, called root-cell, and uses a
%% configuration file associated with it to split off parts of the
%% system's resources and assign them to desired cells. Last but not
%% least, Jailhouse is especially useful since one can circumvent the
%% TrustZone switches in any ARM-adapted OS (e.g., Linux), exploiting the
%% hypervisor calls.

\subsection{Altered communication scheme}
\label{subsec:communication-scheme}

In order to achieve the objective of re-ordering transactions, one
must alter the standard AXI communication scheme explained in the
subsection \ref{subsec:axi_transaction_scheme}.  To this end, we
instantiate a in-between the master and the slave a pass-through
module named SchIM as depicted in figure
\ref{fig:SchIM_transaction_scheme_figure}.  This module is the one in
charge of the re-ordering of the transactions and does so by
intercepting on the fly the transactions emitted by the masters before
they reach the desired slaves.  As shown in the figure
\ref{fig:SchIM_transaction_scheme_figure}, only the phases
instantiated by the masters (i.e. address phase on AW and AR and the
data phase on W) are intercepted for re-ordering by SchIM.  The
introduction of SchIM has a direct consequence on the overall
communication scheme. Indeed, while the response phases on channels R
and B remain unchanged, the address and data phases are duplicated.
Consequently, a write transaction will start exactly as in the
standard AXI scheme with its address phase \circled{1} and data phase
\circled{2}.  These two transactions are buffered within the SchIm
module in \circled{3} and only repeated when decided by the latter's
internal logic.  This release of the transaction leads to the
initialisation of two new address and data phase \circled{4} and
\circled{5}.  Finally, the response phase \circled{6} goes directly
from the slave to the master without being intercepted.  The same
modifications apply to the transmissions of read transactions as the
address phase \circled{1'} is being buffered in \circled{2'} for some
time before being re-emitted in \circled{3'}.  As for the writing, the
response phase \circled{4'} is not intercepted by SchIM.

\begin{figure}
  \centering
  \input{tikz/SchIM_transaction_scheme.tex}
  \caption{Caption}
  \label{fig:SchIM_transaction_scheme_figure}
\end{figure}

\subsection{LLC-\schim Interface and Traffic Accounting}
As illustrated in Fig. \ref{fig:PS-PL-diagram}, the considered system
features an LLC shared between the four cores of the PS. For a
non-cacheable read (resp., write) memory access, which CPU represents
the source of the traffic is carried in the ID bits of the
corresponding AR (resp., AW) AXI transaction. But for cacheable memory
accesses, which is the norm for application workload, this is not the
case. This is mainly because cache controllers typically use a
write-back strategy. In this case, a read or write cache miss causes
up to two events: (1) a cache refill and (2) a cache eviction. The
cache refill is carried out with a read AXI transaction. If the line
being evicted was previously written (dirty), then the eviction causes
a write AXI transaction. It follows that, while read AXI transaction
have an easily identifiable source, write transactions do not. Indeed,
a CPU $x$ might be causing the eviction of a line previously allocated
and modified by CPU $y$. Hence, accounting (and scheduling) the
resulting write transaction as if it originated from CPU $x$ would be
incorrect.

To ensure fair accounting for both read and write traffic, we rely on
cache partitioning through coloring. As studied in a number of
previous works, cache coloring is easy to implement at the hypervisor
level~\cite{biondi_xvisor, cmu_virtpart, detvrt_rtas19}. In our system
setup, we leverage the support Jailhouse already provides. The
standard support has been extended to support booting a Linux inmate
over colored memory. Cache partitioning allows us to establish a
1-to-1 relationship between any read/write transaction traversing the
\schim and the originating CPU. Moreover, with cache coloring in
place, the \schim uses the color bits in the address of the memory
transactions (AR and AW channels) --- instead of the AXI ID bits ---
to differentiate between the traffic of the various cores.

Finally, recall that the \schim forwards transactions between HPM and
HPS ports. These ports follow the asynchronous AXI protocol that
allows issuing multiple outstanding AR and AW transactions. The
protocol dictates that any outstanding transaction must have a unique
AXI ID. This property is crucial to be able to match received
responses with outstanding requests. Unfortunately, there might exist
a mismatch between the bit-width of the AXI ID emitted at the HPM
ports and the bit-width of AXI ID accepted by the HPS ports. For
instance, in the platform of reference, the HPMs emit 16-bit AXI IDs,
while the HPS AXI ID bit-width is 6 bits. Therefore, the \schim also
acts as an AXI ID translator.

In the considered platform, the AXI ID emitted by the ARM Cortex-A53
LLC controller~\cite{ARM-cortex-A53} is the only master for write and
read transactions are \verb|0b1xxxnn| and \verb|0b1xxxx|,
respectively. Here, \verb|x| are variable values and \verb|nn| encode
the core ID. \todo[inline]{RM: incomplete! Please fix with actual IDs}

%% When a cache miss occurs, the LLC controller issues access the
%% targeted memory region by emitting an AXI transaction. The exact size
%% of the transaction (i.e., the amount of burst) will vary depending on
%% the size of the cache lines and the width of the bus. The LLC acting
%% as an intermediate and independent module, it emits transactions that
%% are not directly related to a specific core. For instance, in the case
%% of a cache line eviction, the decision of which line to evict is done
%% by the LLC controller alone.


\subsection{Programming Model}
\begin{table}[!ht]
  \centering
  \caption{Available configuration registers for SchIM}
  \label{tab:configuration_port_structure}
  \begin{tabular}{|c||c|c|}
    \hline
    \multicolumn{1}{|c||}{Field}               & \multicolumn{1}{c|}{Associated Core}      & \multicolumn{1}{c|}{Address}        \\ \hline\hline
    \multirow{4}{*}{TDMA periods}             & $C_{0}$                                   & \verb|base+0x00|                    \\ \cline{2-3} 
    & $C_{1}$                                   & \verb|base+0x04|                    \\ \cline{2-3} 
    & $C_{2}$                                   & \verb|base+0x08|                    \\ \cline{2-3} 
    & $C_{3}$                                   & \verb|base+0x0C|                    \\ \hline
    \multirow{4}{*}{Threshold}                & $C_{0}$                                   & \verb|base+0x10|                    \\ \cline{2-3} 
    & $C_{1}$                                   & \verb|base+0x14|                    \\ \cline{2-3} 
    & $C_{2}$                                   & \verb|base+0x18|                    \\ \cline{2-3} 
    & $C_{3}$                                   & \verb|base+0x1C|                    \\ \hline
    FP Priorities                                & $\{C_{0}, C_{1}, C_{2}, C_{3}\}$          & \verb|base+0x20|                    \\ \hline
    \multirow{4}{*}{MG inter-arrival periods} & $C_{0}$                                   & \verb|base+0x24|                    \\ \cline{2-3} 
    & $C_{1}$                                   & \verb|base+0x28|                    \\ \cline{2-3} 
    & $C_{2}$                                   & \verb|base+0x2C|                    \\ \cline{2-3} 
    & $C_{3}$                                   & \verb|base+0x30|                    \\ \hline
    \multicolumn{3}{|c|}{Reserved}                                                                                              \\ \hline
    Mode                                      & N.A.                                      & \verb|base+0x38|                    \\ \hline
  \end{tabular}
\end{table}

\subsection{Scheduling Interface and Implemented Policies}
All the embedded schedulers included in the scheduling domain share a
common interface, making the integration of a new scheduler
seamless. As inputs, a scheduler module has a manual reset signal that
can be triggered through the configuration port, a vector of bits each
indicating whether the associated queue is empty and a signal
indicating if the latest transaction scheduled as been
consumed. Alongside these inputs, the scheduling modules also have
access to all the configuration registers listed in Table
\ref{tab:configuration_port_structure}. The module has for output the
ID of the queue considered for scheduling and a signal stating whether
the ID is valid.

\subsubsection{Fixed Priority}
The Fixed Priority scheduling module aims at enforcing prioritization
of the traffic of the cores. The priority ordering is explicitly
defined by the user through the configuration port. While SchIM only
has four queues, 16 different levels of priority are offered. Note
that, as it stands, the ordering is strict, meaning that two cores
cannot be assigned with the same priority.

The FP scheduling module only needs two information: the priority
associated with each queue and whether a given queue contains at least
one buffered transaction. Intuitively, the module logic consists in
always considering the highest priority hence, a argmax function is
implemented. However, lower priority queues must also be considered
when higher priority queue do not have transactions. Consequently, the
user defined priority is temporarily considered as 0 when its
corresponding queue does not contain transaction.

\subsubsection{Time Division Multiple Access}
The Time Division Multiple Access (or TDMA) scheduling module provided
by SchIM is a non-work conserving policy that splits the bus usage
between the cores for given periods (also referred to as slots). The
proposed embedded TDMA module enables the user to specify a different
TDMA slot size for each core. The periods are expressed in clock
cycles, enabling a fine grained granularity. The configuration port
enables the end-user to specify and change the periods at runtime.

The implementation of the modules relies on a single register used as
a counter. In fact, we use this register to count the time elapsed in
the current TDMA hyper-period (i.e., the sum of all the cores period)
and is reset to 0 once this hyper-period reached. Alongside this
register, a logic is there to determine in which core slot the counter
actually is and to forward the information to the queue selector. The
logic is able to determine the core to schedule by summing the period
of each previous cores. Provided that the current value of the
counting register is contain between the sum of the previous periods
and the sum of the previous periods and the current one, the
information forwarded to the remaining of the system will be the core
id corresponding to the interval.  \todo[inline]{DH: @DH TODO improve
  description!}

\subsubsection{Traffic Shaping (TS)}
The proposed TS transaction scheduling policy operates by defining a
minimum inter-arrival time that needs to elapse between any two
consecutive transactions from the same CPU. A transaction that arrives
before sufficient time has elapsed since the previous one is held by
the \schim until the aforementioned property is respected.

This scheduling module is implemented as follows. For each of the
\schim queues, the module defines a register counting the time elapsed
since the last forwarded transaction. Once this counter has reached
the period set by the user (through the configuration port), the
module checks if the queue corresponding to the core contains any
transaction. In the case where a transaction is available in the
corresponding queue, the latter is forwarded to the output of SchIM
(i.e., the serializer) and the counting register is reset to
0. Otherwise, the counting register is blocked to the desired period
until a transaction is available for scheduling in the corresponding
queue. Any tie between two cores is solved using a fixed priority
arbitration defined by the user.

The described TS policy is similar to the ARM QoS regulation mechanism
that is available at the level of interconnect for hardware
accelerators~\cite{QoS-301, QoS-400, ewarp_rtss20}. It is also
similar, at least in principle, to software-based memory regulation
techniques such as MemGuard~\ref{memguard}. Yet, TS operates
differently from the hardware implementations of MemGuard-like
regulation that have been proposed so far in~\cite{MITTS,
  Farshchi2020BRUBR}. Indeed, our TS scheduler does not rely on memory
budgets and replenishment periods. Instead, it provides memory
bandwidth enforcement at the granularity of individual memory
transactions.

\subsection{Transactions Life Cycle}
\label{subsec:transaction-life-cycle}

Let us consider a system with four cores $c_0, \ldots, c_3$ sending
transactions $T = \{t_{0}, t_{1}, ..., t_{n}\}$ to the \schim module.
Consequently, the latter boasts four queues (noted $Q = \{q_{0},
q_{1}, q_{2}, q_{3}\}$) buffering the transactions under the form of
packets $P = \{p_{0}, p_{1}, ..., p_{n}\}$ where $p_{i} =
Packetizer(t_{i})~\forall i \in [0 : n]$.

In the present example, we will assume $t_{1}$ as being the
transaction under analysis.  The latter is emitted by $c_{2}$ in
direction of the \schim module.  The packetizer receives this
transaction and, once the AXI protocol completed, transform it into an
equivalent packet $p_{1} = Packetizer(t_{1})$.  Following this
transformation, the newly created packet is forwarded to the
dispatcher which, thanks to the emitter's id embedded within the
transaction, is re-routed to the corresponding queue $q_{2}$ (since
emitted by $c_{2}$).  After the insertion of $p_{1}$ in $q_{2}$, the
state of the queuing domain is as follows: $q_{0}$ has two packets
$p_{0}$ and $p_{k}$ and $q_{2}$ only has $p_{1}$.  At this point,
$q_{0}$ is considered for scheduling by the scheduling domain.  In
consequence, $p_{0}$ is forwarded to the serializer through the
selector.  Simultaneously to the reception of the packet by the
serializer, the latter receives an activation signal from the
scheduling domain informing the serializer that the packet is valid
and that a transaction can be started.  Similarly to the packetizer,
the serializer will transform the packet $p_{0}$ back to its initial
AXI transaction form $t_{0} = Serializer(p_{0})$.  Thereafter, once
the $t_{0}$ has been sent, the serializer will inform the scheduling
domain via a signal, that he is ready to accept the next packet as
input.  Upon the reception of this signal, the scheduling domain will
both re-direct the latter to the queue of the previous packet to
indicate that it has been consumed and change the selected queue
according to the scheduling policy so that the first packet of this
queue can be forwarded to the serializer through the selector module.
In the present example, the "consumed" signal forwarded by the
scheduler is sent to $q_{0}$ which is then empty.  At this instant,
two scenarios are possible:

\begin{enumerate}
\item $q_{0}$ is still considered for scheduling following the
  selected scheduling policy. Therefore, as $q_{0}$ is empty, it
  outputs an "empty" signal received by the scheduling domain.  The
  latter then decides to not send any activation signal to the
  serializer because there is nothing left to transmit in the selected
  queue.  In other words, the access to the main memory is being
  stalled on purpose by the scheduling policy i.e. the scheduling
  policy is not work conserving.  For instance, such a scenario could
  happen in the case of TDMA or if all the queues are empty.  The
  logic will resume as soon as the selected queue is filled.
\item $q_{2}$ is now considered instead of $q_{0}$ for scheduling.  In
  this case, the "consumed" signal is repeated to $q_{0}$ while the
  queue ID changes in order to select $q_{2}$.  This results in the
  packet contained inside $q_{1}$ to be forwarded to the selector.
\end{enumerate}

