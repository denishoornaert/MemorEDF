\section{SchIM Implementation}
    \label{sec:schim_implmentation}

    The present section exposes how SchIM interacts with the remaining of the systems in \ref{subsec:communication-scheme}, how its internal logic enforces transaction scheduling policies in \ref{subsec:micro-arch} and finally, an example of the transaction life cycle within the SchIM module is provided in \ref{subsec:transaction-life-cycle}.

    \subsection{Altered communication scheme}
        \label{subsec:communication-scheme}
        In order to achieve the objective of re-ordering transactions, one must alter the standard AXI communication scheme explained in the subsection \ref{subsec:axi_transaction_scheme}.
        To this end, we instantiate a in-between the master and the slave a pass-through module named SchIM as depicted in figure \ref{fig:SchIM_transaction_scheme_figure}.
        This module is the one in charge of the re-ordering of the transactions and does so by intercepting on the fly the transactions emitted by the masters before they reach the desired slaves.
        As shown in the figure \ref{fig:SchIM_transaction_scheme_figure}, only the phases instantiated by the masters (i.e. address phase on AW and AR and the data phase on W) are intercepted for re-ordering by SchIM.
        The introduction of SchIM has a direct consequence on the overall communication scheme. Indeed, while the response phases on channels R and B remain unchanged, the address and data phases are duplicated.
        Consequently, a write transaction will start exactly as in the standard AXI scheme with its address phase \circled{1} and data phase \circled{2}.
        These two transactions are buffered within the SchIm module in \circled{3} and only repeated when decided by the latter's internal logic.
        This release of the transaction leads to the initialisation of two new address and data phase \circled{4} and \circled{5}.
        Finally, the response phase \circled{6} goes directly from the slave to the master without being intercepted.
        The same modifications apply to the transmissions of read transactions as the address phase \circled{1'} is being buffered in \circled{2'} for some time before being re-emitted in \circled{3'}.
        As for the writing, the response phase \circled{4'} is not intercepted by SchIM.

        \begin{figure}
            \centering
            \input{tikz/SchIM_transaction_scheme.tex}
            \caption{Caption}
            \label{fig:SchIM_transaction_scheme_figure}
        \end{figure}

        
    \subsection{Embedded Schedulers}
        
        \subsubsection{Fixed Priority}
            The Fixed Priority scheduling module aims at enforcing prioritization of the traffic of the cores. The priority ordering is explicitly defined by the user through the configuration port. While SchIM only has four queues, 16 different levels of priority are offered. Note that, as it stands, the ordering is strict, meaning that two cores cannot be assigned with the same priority.
          
            The FP scheduling module only needs two information: the priority associated with each queue and whether a given queue contains at least one buffered transaction. Intuitively, the module logic consists in always considering the highest priority hence, a argmax function is implemented. However, lower priority queues must also be considered when higher priority queue do not have transactions. Consequently, the user defined priority is temporarily considered as 0 when its corresponding queue does not contain transaction.
        
        \subsubsection{Time Division Multiple Access}
            The Time Division Multiple Access (or TDMA) scheduling module provided by SchIM is a non-work conserving policy that splits the bus usage between the cores for given periods (also referred to as slots). The proposed embedded TDMA module enables the user to specify a different TDMA slot size for each core. The periods are expressed in clock cycles, enabling a fine grained granularity. The configuration port enables the end-user to specify and change the periods at runtime.
            
            The implementation of the modules relies on a single register used as a counter. In fact, we use this register to count the time elapsed in the current TDMA hyper-period (i.e., the sum of all the cores period) and is reset to 0 once this hyper-period reached. Alongside this register, a logic is there to determine in which core slot the counter actually is and to forward the information to the queue selector. The logic is able to determine the core to schedule by summing the period of each previous cores. Provided that the current value of the counting register is contain between the sum of the previous periods and the sum of the previous periods and the current one, the information forwarded to the remaining of the system will be the core id corresponding to the interval.
            \todo[inline]{DH: @DH TODO improve description!}
        
        \subsubsection{MemGuard}
            The proposed MemGuard transaction scheduling policy is inspired by the Software-based bus regulation technique ??. The latter has been proved to ensure memory isolation for all the cores involved. Unlike the original MemGuard, the proposed version does not rely on memory budgets and replenishment periods. Instead, it enforces inter-arrival time (i.e. a guaranteed minimal period) between two consecutive transactions emitted by a given core. This distinguishes our approach from \cite{Farshchi2020BRUBR}.
                    
            This scheduling module is implemented as follows. For each of the queues to schedule, the module has a register counting the time elapsed since the start of the period. Once this counter has reached the period set by the user (through the configuration port), the module checks if the queue corresponding to the core contains any transaction. In the case where a transaction is available in the corresponding queue, the latter is forwarded to the output of SchIM (i.e., the serializer) and the counting register is reset to 0. Otherwise, the counting register is blocked to the desired period until a transaction is available for scheduling in the corresponding queue, leading to the counting register to be reset to 0. Any tie between two cores is solved using a fixed priority arbitration defined by the user thanks to the configuration port. 

