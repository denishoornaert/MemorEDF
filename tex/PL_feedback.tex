\section{PL-to-PS Feedback}    
    While the combination of the PS and the PL sides offer great opportunities, it also comes with challenges. In fact, each of the HPM ports interfacing the PS and the PL sides (HPM0 and HPM1) have two dedicated queues for read and write transactions. Since transactions are being buffered inside SchIm as well as in these port buffers, head-of-the-line blocking can happen. Head-of-the-line blocking can be armful for performance or simply cancel all the efforts put in place to enforce transaction reordering and core isolation. For instance, in the case of a non work-conserving policy (e.g., TDMA), if the HPM port queue gets filled with transaction coming for the same core, no other transaction will be able to reach the SchIM and thus be considered for scheduling. This implies that no transaction would be scheduled until the TDMA slot of the core, leading to a worst case scenario of having to wait for a full TDMA hyperperiod. On the other hand, for work-conserving policies (e.g., FP), the decisions being taken bny SchIM would be totally dependent on the order at which transactions are being emitted by the HPM port buffer, cancelling the policy.
    
    In both cases, one might wish to prevent cores from saturating the HPM port buffers. In order to avoid such situation, we implemented a feedback scheme aiming at slowing down the cores. More accurately, the SchIM architecture presented in \ref{sec:schim_implmentation} has been extended with each of the queues being associated with a comparator comparing the current amount of buffered transactions with a threshold defined by the user through the configuration port. If the threshold is reached, an interrupt is sent at the same clock cycle to the PS side. Each core has a dedicated interrupt line considered as a FIQ. The FIQ is handled by a routine in the hypervisor that set a DBS instruction (or Data Barrier Synchronisation) in order to make sure that will not perform any new transaction as long as all the pending transaction have not been served.
    
    Ideally, these buffers should be shared evenly amongst the cores. In our case, since each HPM port has a buffer with a depth of 8 for each type of transactions, each core should occupy at most 2 slots in each buffer. Unfortunately, from our experiments, the control of the exact amount of transactions for each core in the buffers is coarsed. Most of the time, the threshold will be exceeded by one or two transactions. By adding another slave port on which we connect the second available HPM port (namely, HPM1) and assigning two cores two each of the two ports, the ideal amount of transaction from each core being buffered can be doubled, letting us with a bigger margin of error. This is the reason why SchIM is features two slave ports eventhough one is enough.
    
    \todo[inline]{DH: Explain how, intuitively, we can choose the values for each proposed Scheduling policies.}
