\section{Related Work}
There is a broad consensus that memory resources represent the main
performance bottleneck in modern multi-core processors. The
observation has sparked a host of research works addressing the
problem from multiple angles~\cite{temp_reg_survey}. In this context,
the works representing the inspiration for our \schim fall in two
macro-categories, namely \textbf{hardware-based} and
\textbf{software-based} techniques for main memory traffic management.

The first category includes a large body of works aimed at achieving
better and/or more predictable performance by advancing novel hardware
redesigns. The works in \cite{mutlu2007stall, mutlu2008parallelism,
  nesbit2006fair} strive to construct high-performance and fair memory
schedulers. The addition of software-controlled memory deadlines and
transactional semantics where explored in~\cite{usui2016dash} and
\cite{ferri2011soc}, respectively. Next, the work by Ã…kesson et
al.~\cite{akesson2007predator, akesson2010predictable} and Paolieri et
al.~\cite{paolieri2009analyzable} attains timing predictability
through careful scheduling of SDRAM commands. Finally, the MEDUSA DRAM
controller~\cite{valsan2015medusa, detmem2018} implements a two-tiers
scheduler at the DRAM controller to ensure predictability when
accessing memory areas where access time strongly impact application
performance. Finally, the hardware designs proposed in
\cite{zhou2016mitts, rafique2007effective, Farshchi2020BRUBR} put
their emphasis on main memory bandwidth partitioning; clever dynamic
pipelining is further explored in~\cite{drambulism2020rtas} to better
balance average performance and determinism.

Among the software-based techniques are the mechanisms that stemmed
from MemGuard, originally proposed in~\cite{memguard2013} and that
rely on broadly available performance counters to regulate the
bandwidth extracted by individual CPUs. Later extensions to jointly
consider regulation and cache partitioning~\cite{holistic2019rtas} and
to expose control over memory bandwidth as a lockable
resource~\cite{bwlockyun2017} were proposed. Software-based memory
throttling has also been implemented at the
hypervisor-level~\cite{xvisor2018, ewarp2020rtss}. Remarkably, the
work in~\cite{ewarp2020rtss} combines regulation mechanisms for CPU
and embedded accelerators through the ARM QoS
extensions~\cite{qos-400}.

In addition to the two categories surveyed above, perhaps the most
closely related works are those that explored memory isolation
techniques in PS-PL platforms. The work
in~\cite{gracioli2019designing} demonstrated that the PL-side can be
used to define private memory storage, control, and bus units to strongly isolate high-criticality workload. A number of techniques
developed as part of the FRED framework~\cite{fred_ssup} put an
emphasis on memory traffic arbitration and management for in-PL
accelerators~\cite{fred_hyperconnect, fred_abe}. The AXI
HyperConnect~\cite{fred_hyperconnect} is perhaps the component most
similar to the \schim in terms of high-level design. However, both are
substantially different as the \schim is designed to manage
embedded CPUs' memory traffic.

Compared to the literature reviewed above, what sets this work apart
are the following aspects. (1) Our \schim applies to existing PS-PL
commercial systems without introducing any hardware modification; (2)
it allows management in the PL of memory traffic originated by the
embedded CPUs residing in the PS; (3) it provides the framework to
test the feasibility and performance of custom memory scheduling
policies; and (4) it is designed such that multiple schedulers can
coexist, be activated, and configured at runtime.

%% Other works such as~\cite{memguard2013, bwlockyun2017, xvisor2018}
%% focus on implementable mechanisms to regulate/throttle the bandwidth
%% of other low criticality tasks with the goal of reducing contention
%% and improving performance isolation. The first work in this sense
%% was~\cite{memguard2013}, where budget-based bandwidth enforcement is
%% proposed. The work in~\cite{bwlockyun2017} builds on this technique by
%% allowing high-priority tasks to acquire a ``bandwidth lock'' on the
%% memory controller. These techniques have also been shown to be
%% implementable at the hypervisor level~\cite{xvisor2018,
%%   cachepart}. Recently, there have been important efforts to control,
%% account, and ultimately integrate the behavior of accelerators into
%% real-time systems. The work in \cite{biondifpga2017} lays the
%% groundwork for managing hardware accelerator defined in FPGA,
%% while~\cite{houdek2017towards} touches on the topic of non-CPU
%% components regulated via platform-specific throttling mechanisms. In
%% many ways, \sname builds on top of the seminal results achieved in
%% this context and complements the CPU-centric management by integrating
%% traditional accelerators (e.g., DMAs, GPUs) in the picture.
%% mostly at the controller level. Hardware scheduler typically
%% accelerates system performance at the cost of grown hardware
%% resources, inflexibility, and integration hassle. 


\begin{comment}
\todo[inline]{Ask Tomasz and Gero for their ECRTS article}
\todo[inline]{We should look at this paper "BRU: Bandwidth Regulation
  Unit for Real-Time Multicore Processors"} Citation list:
    \begin{itemize}
            \item PREM
            \item MemGuard
            \item PLIM
            \item Three Phase Model
            \item FRED Framework from Sant'Anna
     \end{itemize}
A plethora of techniques have approached by proposing software-only resolutions to alleviate performance interference.
SOFTWARE: ATLAS? / TCM -> fairness / OS-level techniques 
\todo[inline]{SR: to be completed}

Nevertheless, recently, with the advent of the fresh class of commercially available SoCs, integrated with a programmable logic (PL), there is a new third class of regulation.
\todo[inline]{SR: to be completed}
\end{comment}

