--- mm/mmap.c
+++ mm/mmap.c
@@ -1447,6 +1447,14 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 			vm_flags |= VM_NORESERVE;
 	}
 
+    if (flags & MAP_CACHED) {
+        vm_flags |= VM_CACHED;
+    }
+
+    if (flags & MAP_OC_INC) {
+        vm_flags |= VM_OC_INC;
+    }
+
 	addr = mmap_region(file, addr, len, vm_flags, pgoff);
 	if (!IS_ERR_VALUE(addr) &&
 	    ((vm_flags & VM_LOCKED) ||
@@ -1582,6 +1590,37 @@ static inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)
 	return (vm_flags & (VM_NORESERVE | VM_SHARED | VM_WRITE)) == VM_WRITE;
 }
 
+struct page_change_data {
+    pgprot_t set_mask;
+    pgprot_t clear_mask;
+};
+
+static int change_page_range(pte_t *ptep, pgtable_t token, unsigned long addr,
+                                     void *data)
+{
+    struct page_change_data *cdata = data;
+    pte_t pte = *ptep;
+
+    pte = clear_pte_bit(pte, cdata->clear_mask);
+    pte = set_pte_bit(pte, cdata->set_mask);
+
+    set_pte(ptep, pte);
+    return 0;
+}
+
+static int __change_memory_common_mm(struct vm_area_struct * vma,
+        pgprot_t set_mask, pgprot_t clear_mask)
+{
+    struct page_change_data data;
+    int ret;
+
+    data.set_mask = set_mask;
+    data.clear_mask = clear_mask;
+    ret = apply_to_page_range(vma->vm_mm, vma->vm_start,
+            vma->vm_end - vma->vm_start, change_page_range, &data);
+    return ret;
+}
+
 unsigned long mmap_region(struct file *file, unsigned long addr,
 		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff)
 {
@@ -1724,6 +1763,14 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 
 	vma_set_page_prot(vma);
 
+    if (vm_flags & VM_CACHED) {
+        __change_memory_common_mm(vma, __pgprot(PTE_ATTRINDX(MT_NORMAL)), __pgprot(PTE_ATTRINDX(MT_MASK)));
+    }
+
+    if (vm_flags & VM_OC_INC) {
+        __change_memory_common_mm(vma, __pgprot(PTE_ATTRINDX(MT_NORMAL_OC_INC)), __pgprot(PTE_ATTRINDX(MT_MASK)));
+    }
+
 	return addr;
 
 unmap_and_free_vma:
